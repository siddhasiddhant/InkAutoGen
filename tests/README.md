# InkAutoGen Test Suite

Professional testing framework for the InkAutoGen SVG automation extension.

## ğŸ“ Overview

This test suite provides comprehensive coverage for all InkAutoGen functionality after code optimization. The suite is designed for both developers and testers to verify system behavior, identify regressions, and validate new features.

## ğŸ—ï¸ Structure

```
tests/
â”œâ”€â”€ README.md                          # This file - Test suite guide
â”œâ”€â”€ run_tests.py                       # Main test runner with CLI
â”œâ”€â”€ conftest.py                         # PyTest configuration and fixtures
â”œâ”€â”€ unit/                               # Unit tests for individual modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_config.py                 # Configuration constants
â”‚   â”œâ”€â”€ test_common_utils.py            # Common utilities
â”‚   â”œâ”€â”€ test_svg_processor.py          # SVG processing
â”‚   â”œâ”€â”€ test_csv_reader.py              # CSV reading
â”‚   â”œâ”€â”€ test_file_exporter.py           # File exporting
â”‚   â”œâ”€â”€ test_security.py                # Security validation
â”‚   â””â”€â”€ test_performance.py             # Performance utilities
â”œâ”€â”€ integration/                         # End-to-end integration tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_basic_workflow.py         # Basic CSVâ†’SVG workflow
â”‚   â”œâ”€â”€ test_layer_visibility.py        # Layer operations
â”‚   â”œâ”€â”€ test_image_processing.py        # Image handling
â”‚   â”œâ”€â”€ test_property_modification.py  # Style modifications
â”‚   â””â”€â”€ test_error_handling.py          # Error scenarios
â”œâ”€â”€ fixtures/                            # Test data and assets
â”‚   â”œâ”€â”€ svg_templates/                 # Sample SVG files
â”‚   â”œâ”€â”€ csv_samples/                   # Sample CSV files
â”‚   â””â”€â”€ test_data/                     # Generated test assets
â””â”€â”€ utils/                               # Test utilities
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_helpers.py                # Shared test utilities
    â”œâ”€â”€ assertions.py                  # Custom assertions
    â””â”€â”€ mocks.py                      # Test doubles
```

## ğŸš€ Quick Start

### For Developers
```bash
# Run all tests with verbose output
python tests/run_tests.py --verbose

# Run specific test category
python tests/run_tests.py --unit
python tests/run_tests.py --integration
python tests/run_tests.py --security

# Run specific test file
python tests/run_tests.py unit/test_svg_processor.py
```

### For Testers/QA
```bash
# Run with detailed reporting
python tests/run_tests.py --report --output test_report.html

# Run with coverage analysis
python tests/run_tests.py --coverage --min-coverage 80

# Run performance benchmarks
python tests/run_tests.py --benchmark
```

## ğŸ“Š Test Categories

### Unit Tests (`unit/`)
- **Configuration Tests**: Validate all constants and settings
- **Common Utils Tests**: Test reusable utility functions
- **SVG Processor Tests**: Test SVG element manipulation
- **CSV Reader Tests**: Test CSV parsing and validation
- **File Exporter Tests**: Test export functionality
- **Security Tests**: Test file validation and security
- **Performance Tests**: Test caching and optimization

### Integration Tests (`integration/`)
- **Basic Workflow**: Complete CSVâ†’SVG export process
- **Layer Operations**: Layer visibility and management
- **Image Processing**: Image replacement and validation
- **Property Modification**: Style and attribute changes
- **Error Handling**: Exception scenarios and recovery

## ğŸ¯ Key Features

### Test Runner CLI
- **Multiple execution modes**: Unit, integration, all
- **Flexible filtering**: By module, category, or keyword
- **Detailed reporting**: Console, HTML, JSON formats
- **Coverage analysis**: Line and branch coverage with thresholds
- **Performance monitoring**: Execution time and memory usage
- **Continuous integration**: JUnit XML output for CI/CD

### Test Data Management
- **Fixtures**: Reusable test assets and data
- **Parameterized tests**: Data-driven test execution
- **Mock support**: External dependency isolation
- **Temporary environments**: Isolated test execution
- **Cleanup automation**: Post-test resource management

## ğŸ“ˆ Coverage Areas

### Core Functionality
- âœ… Configuration consolidation
- âœ… Common utility functions
- âœ… SVG element processing
- âœ… CSV data reading
- âœ… File export operations
- âœ… Security validation
- âœ… Error handling patterns

### Edge Cases
- âœ… Invalid input handling
- âœ… Malformed file recovery
- âœ… Performance edge cases
- âœ… Memory constraint scenarios
- âœ… Concurrent access patterns

### Integration Scenarios
- âœ… Complete workflows
- âœ… Cross-module interactions
- âœ… Real-world file processing
- âœ… Error propagation
- âœ… Performance characteristics

## ğŸ”§ Configuration

### Environment Variables
```bash
INKAUTOGEN_TEST_DATA_DIR="/path/to/test/fixtures"
INKAUTOGEN_TEST_OUTPUT_DIR="/tmp/inkautogen_tests"
INKAUTOGEN_TEST_TIMEOUT=30                    # seconds
INKAUTOGEN_TEST_PARALLEL=4                     # worker threads
```

### Configuration Files
- `tests/test_config.json` - Test execution settings
- `tests/coverage_config.json` - Coverage requirements
- `tests/benchmark_config.json` - Performance baselines

## ğŸ“‹ Test Examples

### Basic SVG Processing Test
```python
# Using the test framework
from tests.utils.test_helpers import create_test_svg, assert_svg_modification
from tests.unit.test_svg_processor import TestSVGProcessor

def test_custom_scenario():
    # Create test SVG
    svg_content = create_test_svg("template_with_layers.svg")
    
    # Process modification
    processor = TestSVGProcessor()
    result = processor.process_text_element(svg_root, "TestText", "New Value")
    
    # Assert result
    assert_svg_modification(result, expected_changes={"TestText": "New Value"})
```

### Integration Test Example
```python
# Complete workflow test
from tests.integration.test_basic_workflow import TestBasicWorkflow

def test_csv_to_svg_export():
    workflow = TestBasicWorkflow()
    
    # Setup test data
    test_data = workflow.create_test_csv("sample_data.csv")
    test_template = workflow.create_svg_template("template.svg")
    
    # Execute workflow
    results = workflow.execute_export(test_data, test_template)
    
    # Validate results
    workflow.assert_export_success(results)
    workflow.assert_output_files_created(results)
```

## ğŸš¦ Usage Guidelines

### For Adding New Tests
1. **Create test file** in appropriate category (`unit/` or `integration/`)
2. **Follow naming convention**: `test_<functionality>.py`
3. **Inherit from base classes**: Use provided test utilities
4. **Use fixtures**: Place test data in `fixtures/` directory
5. **Document purpose**: Add docstring explaining test scenario
6. **Include assertions**: Use framework's assertion helpers

### For Running Tests
1. **Environment setup**: Ensure test dependencies installed
2. **Database isolation**: Use separate test database/cache
3. **File cleanup**: Tests should not leave artifacts
4. **Parallel execution**: Use provided parallel test runner
5. **Result analysis**: Review detailed reports for insights

## ğŸ“š Documentation References

- **Test Results**: `tests/reports/` directory
- **Coverage Reports**: `tests/coverage/` directory
- **Benchmark Data**: `tests/benchmarks/` directory
- **API Documentation**: Individual test file docstrings
- **Examples**: `tests/examples/` directory

## ğŸ” Troubleshooting

### Common Issues
- **Import errors**: Check `PYTHONPATH` includes project root
- **Fixture not found**: Verify file paths in test fixtures
- **Timeout failures**: Increase `INKAUTOGEN_TEST_TIMEOUT`
- **Memory issues**: Reduce `INKAUTOGEN_TEST_PARALLEL`

### Debug Mode
```bash
# Run with maximum debugging output
python tests/run_tests.py --debug --verbose --no-cleanup

# Individual test debugging
python tests/run_tests.py --debug --test test_svg_processor::test_text_replacement
```

## ğŸ¯ Success Criteria

### Test Suite Success
- âœ… All unit tests pass (>95% coverage)
- âœ… All integration tests complete successfully
- âœ… No new regressions introduced
- âœ… Performance within acceptable thresholds
- âœ… Security validation effective
- âœ… Documentation completeness â‰¥ 90%
- âœ… All tests reproducible

### Quality Gates
- âœ… Code coverage â‰¥ 80%
- âœ… No critical vulnerabilities
- âœ… Performance regression â‰¤ 5%
- âœ… All tests reproducible
- âœ… Professional test documentation

---

This test suite ensures that code optimizations maintain functionality while providing comprehensive validation for both development and testing workflows.